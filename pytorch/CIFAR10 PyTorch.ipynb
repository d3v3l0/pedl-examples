{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 Tensorflow Pytorch\n",
    "\n",
    "This notebook walks you through image classification model training in PEDL on the popular [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), specifically using the PyTorch machine learning library.  See [this notebook](../tf-keras/CIFAR10%20Tensorflow%20Keras.ipynb) for the same example built on TensorFlow Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test importing PEDL.  In PEDL is properly installed, you should see no output.\n",
    "import pedl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace with the IP address of the PEDL master.\n",
    "pedl_master = '<master IP>'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run an Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will explore the components of a PEDL experiment; namely, the model definition and associated experiment configuration.\n",
    "\n",
    "## Model Directory\n",
    "- `__init__.py`: The entrypoint for PEDL to resolve the experiment `Trial` interface\n",
    "- `data.py`: The data loading interface implementation\n",
    "- `model_def.py`: The PyTorch model definition\n",
    "- `.yaml` configuration files that each govern an individual experiment run\n",
    "\n",
    "Let's look at the contents of the model directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls cifar10_cnn_pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data_py'></a>\n",
    "### data.py\n",
    "Take a look at the data loading functionality in `data.py`.  The `make_data_loaders` function is a required function that specifies the training and validation datasets to use in an experiment, while `download_data` is an optional performance efficiency API which, if implemented, causes PEDL to download and cache data once per machine rather than per process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n cifar10_cnn_pytorch/data.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_def.py\n",
    "Now drill in and view the model definition file.  Look for the implementation of PEDL's `PyTorchTrial` interface.  This is the interface between PEDL and PyTorch, which ultimately enables the ML Engineer to leverage PEDL's distributed hyperparameter search in a shared runtime without having to worry about these distributed system concerns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat -n cifar10_cnn_pytorch/model_def.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_\\_init\\_\\_.py\n",
    "Given that a PEDL model definition is a Python package, `__init__.py` is the entrypoint that exposes the `Trial` implementation and data loading methods we just explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n cifar10_cnn_pytorch/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### const.yaml\n",
    "For our first PEDL experiment, we'll run this model training job with fixed hyperparameters.  Note the following sections:\n",
    "- `description`: A short description of the experiment\n",
    "- `data`: A section for user to provide custom key value pairs.  Here we specify where the data resides.  Note its usage in [data.py](#data_py) on line 32.\n",
    "- `hyperparameters`: area for user to define hyperparameters that will be injected into the trial class at runtime. There are constant values for this configuration\n",
    "- `searcher`: hyperparameter search algorithm for the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n cifar10_cnn_pytorch/const.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pedl -m {pedl_master} experiment create cifar10_cnn_pytorch/const.yaml cifar10_cnn_pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the experiment completes (which may take a few minutes if PEDL agents have to start up), look at the experiment page to see the single completed trial.  Note the validation error around 0.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive Hyperparameter Search\n",
    "### adaptive.yaml\n",
    "\n",
    "Next, let's run an experiment with the same model definition, but we'll leverage PEDL's adaptive hyperparameter search to efficiently determine the hyperparameter values that yield the lowest validation error.  Note that hyperparameters in the experiment configuration are specified as ranges as opposed to fixed values as in our [first experiment](#const.yaml)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat -n cifar10_cnn_pytorch/adaptive.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pedl -m {pedl_master} experiment create cifar10_cnn_pytorch/adaptive.yaml cifar10_cnn_pytorch/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During and after the experiment run, you can view the best (lowest) validation error that PEDL's adaptive search finds over time:\n",
    "\n",
    "![adaptive](img/adaptive_pytorch.png)\n",
    "\n",
    "When the experiment finishes, note that your best performing model achieves a lower validation error than our first experiment that ran with constant hyperparameter values.  From the PEDL experiment detail page, you can drill in to a particular trial and view the hyperparameter values used.  You can also access the saved checkpoint of your best-performing model and load it for real-time or batch inference as described in the PyTorch documentation [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-a-general-checkpoint-for-inference-and-or-resuming-training)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
